| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(batch_size=400, epochs=30, update_freq=1, save_ckpt_freq=5, robust_test=None, model='EEGPT', qkv_bias=True, rel_pos_bias=False, abs_pos_emb=True, layer_scale_init_value=0.1, input_size=200, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.9, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, smoothing=0.1, reprob=0.25, remode='pixel', recount=1, resplit=False, finetune='/home/aa2650/playground/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt', model_key='model|module|state_dict', model_prefix='', model_filter_name='gzp', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, nb_classes=0, output_dir='./checkpoints_TUEV/finetune_tuev_eegpt/', log_dir='./log/finetune_tuev_eegpt', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=True, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, dataset='TUEV', rank=0, gpu=0, distributed=True, dist_backend='nccl')
65967 17965 28305
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x76b0d15babf0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Patch size = 200
Load ckpt from /home/aa2650/playground/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt
Weights of EEGPTClassifier not initialized from pretrained model: ['chan_conv.0.weight', 'chan_conv.0.bias', 'chan_conv.1.weight', 'chan_conv.1.bias', 'chan_conv.1.running_mean', 'chan_conv.1.running_var', 'chan_conv.3.weight', 'chan_conv.3.bias', 'chan_conv.4.weight', 'chan_conv.4.bias', 'chan_conv.4.running_mean', 'chan_conv.4.running_var', 'reconstructor.cls_token', 'fc_norm.weight', 'fc_norm.bias', 'head.1.weight', 'head.1.bias']
Weights from pretrained model not used in EEGPTClassifier: ['encoder.summary_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.chan_embed.weight', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'predictor.mask_token', 'predictor.predictor_embed.weight', 'predictor.predictor_embed.bias', 'predictor.time_embed.freqs', 'predictor.predictor_blocks.0.norm1.weight', 'predictor.predictor_blocks.0.norm1.bias', 'predictor.predictor_blocks.0.attn.qkv.weight', 'predictor.predictor_blocks.0.attn.qkv.bias', 'predictor.predictor_blocks.0.attn.proj.weight', 'predictor.predictor_blocks.0.attn.proj.bias', 'predictor.predictor_blocks.0.norm2.weight', 'predictor.predictor_blocks.0.norm2.bias', 'predictor.predictor_blocks.0.mlp.fc1.weight', 'predictor.predictor_blocks.0.mlp.fc1.bias', 'predictor.predictor_blocks.0.mlp.fc2.weight', 'predictor.predictor_blocks.0.mlp.fc2.bias', 'predictor.predictor_blocks.1.norm1.weight', 'predictor.predictor_blocks.1.norm1.bias', 'predictor.predictor_blocks.1.attn.qkv.weight', 'predictor.predictor_blocks.1.attn.qkv.bias', 'predictor.predictor_blocks.1.attn.proj.weight', 'predictor.predictor_blocks.1.attn.proj.bias', 'predictor.predictor_blocks.1.norm2.weight', 'predictor.predictor_blocks.1.norm2.bias', 'predictor.predictor_blocks.1.mlp.fc1.weight', 'predictor.predictor_blocks.1.mlp.fc1.bias', 'predictor.predictor_blocks.1.mlp.fc2.weight', 'predictor.predictor_blocks.1.mlp.fc2.bias', 'predictor.predictor_blocks.2.norm1.weight', 'predictor.predictor_blocks.2.norm1.bias', 'predictor.predictor_blocks.2.attn.qkv.weight', 'predictor.predictor_blocks.2.attn.qkv.bias', 'predictor.predictor_blocks.2.attn.proj.weight', 'predictor.predictor_blocks.2.attn.proj.bias', 'predictor.predictor_blocks.2.norm2.weight', 'predictor.predictor_blocks.2.norm2.bias', 'predictor.predictor_blocks.2.mlp.fc1.weight', 'predictor.predictor_blocks.2.mlp.fc1.bias', 'predictor.predictor_blocks.2.mlp.fc2.weight', 'predictor.predictor_blocks.2.mlp.fc2.bias', 'predictor.predictor_blocks.3.norm1.weight', 'predictor.predictor_blocks.3.norm1.bias', 'predictor.predictor_blocks.3.attn.qkv.weight', 'predictor.predictor_blocks.3.attn.qkv.bias', 'predictor.predictor_blocks.3.attn.proj.weight', 'predictor.predictor_blocks.3.attn.proj.bias', 'predictor.predictor_blocks.3.norm2.weight', 'predictor.predictor_blocks.3.norm2.bias', 'predictor.predictor_blocks.3.mlp.fc1.weight', 'predictor.predictor_blocks.3.mlp.fc1.bias', 'predictor.predictor_blocks.3.mlp.fc2.weight', 'predictor.predictor_blocks.3.mlp.fc2.bias', 'predictor.predictor_blocks.4.norm1.weight', 'predictor.predictor_blocks.4.norm1.bias', 'predictor.predictor_blocks.4.attn.qkv.weight', 'predictor.predictor_blocks.4.attn.qkv.bias', 'predictor.predictor_blocks.4.attn.proj.weight', 'predictor.predictor_blocks.4.attn.proj.bias', 'predictor.predictor_blocks.4.norm2.weight', 'predictor.predictor_blocks.4.norm2.bias', 'predictor.predictor_blocks.4.mlp.fc1.weight', 'predictor.predictor_blocks.4.mlp.fc1.bias', 'predictor.predictor_blocks.4.mlp.fc2.weight', 'predictor.predictor_blocks.4.mlp.fc2.bias', 'predictor.predictor_blocks.5.norm1.weight', 'predictor.predictor_blocks.5.norm1.bias', 'predictor.predictor_blocks.5.attn.qkv.weight', 'predictor.predictor_blocks.5.attn.qkv.bias', 'predictor.predictor_blocks.5.attn.proj.weight', 'predictor.predictor_blocks.5.attn.proj.bias', 'predictor.predictor_blocks.5.norm2.weight', 'predictor.predictor_blocks.5.norm2.bias', 'predictor.predictor_blocks.5.mlp.fc1.weight', 'predictor.predictor_blocks.5.mlp.fc1.bias', 'predictor.predictor_blocks.5.mlp.fc2.weight', 'predictor.predictor_blocks.5.mlp.fc2.bias', 'predictor.predictor_blocks.6.norm1.weight', 'predictor.predictor_blocks.6.norm1.bias', 'predictor.predictor_blocks.6.attn.qkv.weight', 'predictor.predictor_blocks.6.attn.qkv.bias', 'predictor.predictor_blocks.6.attn.proj.weight', 'predictor.predictor_blocks.6.attn.proj.bias', 'predictor.predictor_blocks.6.norm2.weight', 'predictor.predictor_blocks.6.norm2.bias', 'predictor.predictor_blocks.6.mlp.fc1.weight', 'predictor.predictor_blocks.6.mlp.fc1.bias', 'predictor.predictor_blocks.6.mlp.fc2.weight', 'predictor.predictor_blocks.6.mlp.fc2.bias', 'predictor.predictor_blocks.7.norm1.weight', 'predictor.predictor_blocks.7.norm1.bias', 'predictor.predictor_blocks.7.attn.qkv.weight', 'predictor.predictor_blocks.7.attn.qkv.bias', 'predictor.predictor_blocks.7.attn.proj.weight', 'predictor.predictor_blocks.7.attn.proj.bias', 'predictor.predictor_blocks.7.norm2.weight', 'predictor.predictor_blocks.7.norm2.bias', 'predictor.predictor_blocks.7.mlp.fc1.weight', 'predictor.predictor_blocks.7.mlp.fc1.bias', 'predictor.predictor_blocks.7.mlp.fc2.weight', 'predictor.predictor_blocks.7.mlp.fc2.bias', 'predictor.predictor_norm.weight', 'predictor.predictor_norm.bias', 'predictor.predictor_proj.weight', 'predictor.predictor_proj.bias']
Model = EEGPTClassifier(
  (chan_conv): Sequential(
    (0): Conv2dWithConstraint(23, 20, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Conv2d(20, 20, kernel_size=(1, 55), stride=(1, 1), padding=same, groups=20)
    (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): Dropout(p=0.8, inplace=False)
  )
  (target_encoder): EEGTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(1, 512, kernel_size=(1, 64), stride=(1, 64))
    )
    (chan_embed): Embedding(62, 512)
    (blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (reconstructor): EEGTransformerReconstructor(
    (reconstructor_embed): Linear(in_features=512, out_features=512, bias=True)
    (time_embed): RotaryEmbedding()
    (chan_embed): Embedding(62, 512)
    (reconstructor_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (reconstructor_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (reconstructor_proj): Linear(in_features=512, out_features=64, bias=True)
  )
  (norm): Identity()
  (fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (head): Sequential(
    (0): Dropout(p=0.8, inplace=False)
    (1): LinearWithConstraint(in_features=30720, out_features=6, bias=True)
  )
)
number of params: 51022550
LR = 0.00050000
Batch size = 800
Update frequent = 1
Number of training examples = 65967
Number of training training per epoch = 82
Assigned values = [0.16677181699666577, 0.18530201888518416, 0.20589113209464907, 0.2287679245496101, 0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay name marked in model: {'reconstructor.chan_embed', 'reconstructor.cls_token', 'target_encoder.chan_embed', 'reconstructor.time_embed', 'reconstructor.pos_embed', 'target_encoder.summary_token'}
Param groups = {
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "chan_conv.0.weight",
      "chan_conv.3.weight",
      "target_encoder.patch_embed.proj.weight",
      "target_encoder.chan_embed.weight",
      "target_encoder.blocks.0.attn.qkv.weight",
      "target_encoder.blocks.0.attn.proj.weight",
      "target_encoder.blocks.0.mlp.fc1.weight",
      "target_encoder.blocks.0.mlp.fc2.weight",
      "target_encoder.blocks.1.attn.qkv.weight",
      "target_encoder.blocks.1.attn.proj.weight",
      "target_encoder.blocks.1.mlp.fc1.weight",
      "target_encoder.blocks.1.mlp.fc2.weight",
      "target_encoder.blocks.2.attn.qkv.weight",
      "target_encoder.blocks.2.attn.proj.weight",
      "target_encoder.blocks.2.mlp.fc1.weight",
      "target_encoder.blocks.2.mlp.fc2.weight",
      "target_encoder.blocks.3.attn.qkv.weight",
      "target_encoder.blocks.3.attn.proj.weight",
      "target_encoder.blocks.3.mlp.fc1.weight",
      "target_encoder.blocks.3.mlp.fc2.weight",
      "target_encoder.blocks.4.attn.qkv.weight",
      "target_encoder.blocks.4.attn.proj.weight",
      "target_encoder.blocks.4.mlp.fc1.weight",
      "target_encoder.blocks.4.mlp.fc2.weight",
      "target_encoder.blocks.5.attn.qkv.weight",
      "target_encoder.blocks.5.attn.proj.weight",
      "target_encoder.blocks.5.mlp.fc1.weight",
      "target_encoder.blocks.5.mlp.fc2.weight",
      "target_encoder.blocks.6.attn.qkv.weight",
      "target_encoder.blocks.6.attn.proj.weight",
      "target_encoder.blocks.6.mlp.fc1.weight",
      "target_encoder.blocks.6.mlp.fc2.weight",
      "target_encoder.blocks.7.attn.qkv.weight",
      "target_encoder.blocks.7.attn.proj.weight",
      "target_encoder.blocks.7.mlp.fc1.weight",
      "target_encoder.blocks.7.mlp.fc2.weight",
      "reconstructor.mask_token",
      "reconstructor.reconstructor_embed.weight",
      "reconstructor.chan_embed.weight",
      "reconstructor.reconstructor_blocks.0.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.0.attn.proj.weight",
      "reconstructor.reconstructor_blocks.0.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.0.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.1.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.1.attn.proj.weight",
      "reconstructor.reconstructor_blocks.1.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.1.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.2.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.2.attn.proj.weight",
      "reconstructor.reconstructor_blocks.2.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.2.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.3.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.3.attn.proj.weight",
      "reconstructor.reconstructor_blocks.3.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.3.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.4.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.4.attn.proj.weight",
      "reconstructor.reconstructor_blocks.4.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.4.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.5.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.5.attn.proj.weight",
      "reconstructor.reconstructor_blocks.5.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.5.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.6.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.6.attn.proj.weight",
      "reconstructor.reconstructor_blocks.6.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.6.mlp.fc2.weight",
      "reconstructor.reconstructor_blocks.7.attn.qkv.weight",
      "reconstructor.reconstructor_blocks.7.attn.proj.weight",
      "reconstructor.reconstructor_blocks.7.mlp.fc1.weight",
      "reconstructor.reconstructor_blocks.7.mlp.fc2.weight",
      "reconstructor.reconstructor_proj.weight",
      "head.1.weight"
    ],
    "lr_scale": 1.0
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "chan_conv.0.bias",
      "chan_conv.1.weight",
      "chan_conv.1.bias",
      "chan_conv.3.bias",
      "chan_conv.4.weight",
      "chan_conv.4.bias",
      "target_encoder.summary_token",
      "target_encoder.patch_embed.proj.bias",
      "target_encoder.blocks.0.norm1.weight",
      "target_encoder.blocks.0.norm1.bias",
      "target_encoder.blocks.0.attn.qkv.bias",
      "target_encoder.blocks.0.attn.proj.bias",
      "target_encoder.blocks.0.norm2.weight",
      "target_encoder.blocks.0.norm2.bias",
      "target_encoder.blocks.0.mlp.fc1.bias",
      "target_encoder.blocks.0.mlp.fc2.bias",
      "target_encoder.blocks.1.norm1.weight",
      "target_encoder.blocks.1.norm1.bias",
      "target_encoder.blocks.1.attn.qkv.bias",
      "target_encoder.blocks.1.attn.proj.bias",
      "target_encoder.blocks.1.norm2.weight",
      "target_encoder.blocks.1.norm2.bias",
      "target_encoder.blocks.1.mlp.fc1.bias",
      "target_encoder.blocks.1.mlp.fc2.bias",
      "target_encoder.blocks.2.norm1.weight",
      "target_encoder.blocks.2.norm1.bias",
      "target_encoder.blocks.2.attn.qkv.bias",
      "target_encoder.blocks.2.attn.proj.bias",
      "target_encoder.blocks.2.norm2.weight",
      "target_encoder.blocks.2.norm2.bias",
      "target_encoder.blocks.2.mlp.fc1.bias",
      "target_encoder.blocks.2.mlp.fc2.bias",
      "target_encoder.blocks.3.norm1.weight",
      "target_encoder.blocks.3.norm1.bias",
      "target_encoder.blocks.3.attn.qkv.bias",
      "target_encoder.blocks.3.attn.proj.bias",
      "target_encoder.blocks.3.norm2.weight",
      "target_encoder.blocks.3.norm2.bias",
      "target_encoder.blocks.3.mlp.fc1.bias",
      "target_encoder.blocks.3.mlp.fc2.bias",
      "target_encoder.blocks.4.norm1.weight",
      "target_encoder.blocks.4.norm1.bias",
      "target_encoder.blocks.4.attn.qkv.bias",
      "target_encoder.blocks.4.attn.proj.bias",
      "target_encoder.blocks.4.norm2.weight",
      "target_encoder.blocks.4.norm2.bias",
      "target_encoder.blocks.4.mlp.fc1.bias",
      "target_encoder.blocks.4.mlp.fc2.bias",
      "target_encoder.blocks.5.norm1.weight",
      "target_encoder.blocks.5.norm1.bias",
      "target_encoder.blocks.5.attn.qkv.bias",
      "target_encoder.blocks.5.attn.proj.bias",
      "target_encoder.blocks.5.norm2.weight",
      "target_encoder.blocks.5.norm2.bias",
      "target_encoder.blocks.5.mlp.fc1.bias",
      "target_encoder.blocks.5.mlp.fc2.bias",
      "target_encoder.blocks.6.norm1.weight",
      "target_encoder.blocks.6.norm1.bias",
      "target_encoder.blocks.6.attn.qkv.bias",
      "target_encoder.blocks.6.attn.proj.bias",
      "target_encoder.blocks.6.norm2.weight",
      "target_encoder.blocks.6.norm2.bias",
      "target_encoder.blocks.6.mlp.fc1.bias",
      "target_encoder.blocks.6.mlp.fc2.bias",
      "target_encoder.blocks.7.norm1.weight",
      "target_encoder.blocks.7.norm1.bias",
      "target_encoder.blocks.7.attn.qkv.bias",
      "target_encoder.blocks.7.attn.proj.bias",
      "target_encoder.blocks.7.norm2.weight",
      "target_encoder.blocks.7.norm2.bias",
      "target_encoder.blocks.7.mlp.fc1.bias",
      "target_encoder.blocks.7.mlp.fc2.bias",
      "target_encoder.norm.weight",
      "target_encoder.norm.bias",
      "reconstructor.cls_token",
      "reconstructor.reconstructor_embed.bias",
      "reconstructor.reconstructor_blocks.0.norm1.weight",
      "reconstructor.reconstructor_blocks.0.norm1.bias",
      "reconstructor.reconstructor_blocks.0.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.0.attn.proj.bias",
      "reconstructor.reconstructor_blocks.0.norm2.weight",
      "reconstructor.reconstructor_blocks.0.norm2.bias",
      "reconstructor.reconstructor_blocks.0.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.0.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.1.norm1.weight",
      "reconstructor.reconstructor_blocks.1.norm1.bias",
      "reconstructor.reconstructor_blocks.1.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.1.attn.proj.bias",
      "reconstructor.reconstructor_blocks.1.norm2.weight",
      "reconstructor.reconstructor_blocks.1.norm2.bias",
      "reconstructor.reconstructor_blocks.1.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.1.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.2.norm1.weight",
      "reconstructor.reconstructor_blocks.2.norm1.bias",
      "reconstructor.reconstructor_blocks.2.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.2.attn.proj.bias",
      "reconstructor.reconstructor_blocks.2.norm2.weight",
      "reconstructor.reconstructor_blocks.2.norm2.bias",
      "reconstructor.reconstructor_blocks.2.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.2.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.3.norm1.weight",
      "reconstructor.reconstructor_blocks.3.norm1.bias",
      "reconstructor.reconstructor_blocks.3.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.3.attn.proj.bias",
      "reconstructor.reconstructor_blocks.3.norm2.weight",
      "reconstructor.reconstructor_blocks.3.norm2.bias",
      "reconstructor.reconstructor_blocks.3.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.3.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.4.norm1.weight",
      "reconstructor.reconstructor_blocks.4.norm1.bias",
      "reconstructor.reconstructor_blocks.4.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.4.attn.proj.bias",
      "reconstructor.reconstructor_blocks.4.norm2.weight",
      "reconstructor.reconstructor_blocks.4.norm2.bias",
      "reconstructor.reconstructor_blocks.4.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.4.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.5.norm1.weight",
      "reconstructor.reconstructor_blocks.5.norm1.bias",
      "reconstructor.reconstructor_blocks.5.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.5.attn.proj.bias",
      "reconstructor.reconstructor_blocks.5.norm2.weight",
      "reconstructor.reconstructor_blocks.5.norm2.bias",
      "reconstructor.reconstructor_blocks.5.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.5.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.6.norm1.weight",
      "reconstructor.reconstructor_blocks.6.norm1.bias",
      "reconstructor.reconstructor_blocks.6.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.6.attn.proj.bias",
      "reconstructor.reconstructor_blocks.6.norm2.weight",
      "reconstructor.reconstructor_blocks.6.norm2.bias",
      "reconstructor.reconstructor_blocks.6.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.6.mlp.fc2.bias",
      "reconstructor.reconstructor_blocks.7.norm1.weight",
      "reconstructor.reconstructor_blocks.7.norm1.bias",
      "reconstructor.reconstructor_blocks.7.attn.qkv.bias",
      "reconstructor.reconstructor_blocks.7.attn.proj.bias",
      "reconstructor.reconstructor_blocks.7.norm2.weight",
      "reconstructor.reconstructor_blocks.7.norm2.bias",
      "reconstructor.reconstructor_blocks.7.mlp.fc1.bias",
      "reconstructor.reconstructor_blocks.7.mlp.fc2.bias",
      "reconstructor.reconstructor_norm.weight",
      "reconstructor.reconstructor_norm.bias",
      "reconstructor.reconstructor_proj.bias",
      "fc_norm.weight",
      "fc_norm.bias",
      "head.1.bias"
    ],
    "lr_scale": 1.0
  }
}
Optimizer config: {'lr': 0.0005, 'weight_decay': 0.0, 'eps': 1e-08}
Use step level LR scheduler!
Set warmup steps = 410
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = LabelSmoothingCrossEntropy()
Auto resume checkpoint: 
Test:  [ 0/24]  eta: 0:01:46  loss: 1.7752 (1.7752)  n: 600.0000 (600.0000)  accuracy: 0.1983 (0.1983)  balanced_accuracy: 0.1428 (0.1428)  cohen_kappa: -0.0019 (-0.0019)  f1_weighted: 0.2210 (0.2210)  time: 4.4178  data: 2.0093  max mem: 3763
Test:  [10/24]  eta: 0:00:11  loss: 1.7666 (1.7631)  n: 600.0000 (600.0000)  accuracy: 0.2017 (0.1995)  balanced_accuracy: 0.1686 (0.1680)  cohen_kappa: 0.0099 (0.0054)  f1_weighted: 0.2255 (0.2216)  time: 0.8561  data: 0.1828  max mem: 3763
Test:  [20/24]  eta: 0:00:02  loss: 1.7567 (1.7590)  n: 600.0000 (600.0000)  accuracy: 0.2017 (0.1995)  balanced_accuracy: 0.1718 (0.1684)  cohen_kappa: 0.0099 (0.0083)  f1_weighted: 0.2255 (0.2215)  time: 0.4999  data: 0.0001  max mem: 3763
Test:  [23/24]  eta: 0:00:00  loss: 1.7567 (1.7590)  n: 600.0000 (589.7083)  accuracy: 0.2000 (0.1996)  balanced_accuracy: 0.1686 (0.1689)  cohen_kappa: 0.0099 (0.0088)  f1_weighted: 0.2217 (0.2214)  time: 0.4963  data: 0.0001  max mem: 3763
Test: Total time: 0:00:15 (0.6637 s / it)
* loss 1.755
{'accuracy': 0.19960432417155374, 'balanced_accuracy': 0.16931513401748277, 'cohen_kappa': 0.008971307463627176, 'f1_weighted': 0.22129722917922898, 'loss': 1.7548610493540764, 'precision': 0.1848731541976667, 'sensitivity': 0.16931513401748277, 'f1': 0.11077520052772032, 'specificity': 0.8397206513294789}
Accuracy of the network on the 28305 test EEG: 0.20%
